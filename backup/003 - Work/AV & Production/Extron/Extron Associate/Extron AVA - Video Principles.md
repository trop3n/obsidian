 
## Overview

The use of video has become an important and powerful means of information in our society and everyday lives. As it moves beyond homes and movie theaters for entertainment, the access to video content is no longer constrained to a permanent location.

Whether video is used for personal use, a corporate presentation at the office, signage messaging in the airport or lobby, or in an education environment, the distribution of content is now readily available almost anywhere. 

The spread of video is greatly attributed to continual enhancements in technology. The AV industry plays an important role in supporting these advancements, while maintaining current or legacy technologies.

This course will examine the fundamentals of video, including the different types of signal formats, interface connectors, and how Extron creates innovative solutions to meet the needs of any video application. 

## Objectives

Upon completion of this course, you will be able to:

- Understand the process of how video is created and its journey to a display device
- Define all of the parameters and basic elements associated with video
- Identify analog and digital signals and associated connector types
- Understand the signal transport process for distribution of video signals
- Identify the various standards organizations that develop guidelines to ensure reliability of video technology
- Recognize the importance of digital content protection and how it is implemented

## What is Video?

> Video is the end result of capturing or recording a series of visual images that when displayed in sequence create moving pictures. 

The human eye can differentiate between individual images at the rate of 10 to 12 frames per second - fps. But is unable to distinguish each image if the rate increases to speeds of 24 to 30 fps. 

## Recording Process

So, how do we get video? There are many ways to capture, create, or generate video. 

In early recordings, there was only one way to capture video. Cameras used a tube to capture live scenes that were focused through a lens onto a photo sensitive plate. An electron beam scanned the entire surface of the plate,  from left to right, and top to bottom repeatedly to capture the brightness and darkness of the scene. 

The black and white camera signals were broadcast for television display or recorded to magnetic tape.

Today, our recording devices consist of cinema and studio cameras, camcorders, webcams, smartphones, tablets, etc. Instead of physically capturing an image, these devices convert what the camera lens sees into a string of ones and zeroes using a light sensitive microchip. 
## Aspect Ratio

The ***Aspect Ratio*** is the comparison of an image's width to it's height. The ratio is denoted in units of measure as opposed to exact pixels. For instance, a 4:3 ratio measures 4 units wide and 3 units in height. If these values are broken down further, they show an approximate 1.33 to 1 relationship. The "original" 4:3 standard of video has been replaced by 16:9 or approximately 1.78:1 wide screen aspect ratio.

With the increased use of video in social media, the following aspect ratios for mobile devices are used:

- 1:1 - Square
- 1.91:1 - Landscape
- 4:5 - Portrait
- 9:16 - Full Screen Portrait
## Resolution

***Resolution*** measures the image size as width x height, in pixels. 

For example, a resolution of 640x480, indicates the width is 640 pixel and the height is 480 pixels.

Full HD resolution starts at 1920x1080, and increases upwards to 3840x2160, or 4K, then 7680x4320, also known as 8K. 10K resolution is the top end at 10240x4320.

What is a ***Pixel?*** A singular point in a digital image. The more pixels in an image, the more detailed and refined the image will appear. 
## Source, Distribution and Destination

Video is generated by a variety of **source** devices, such as a video camera, desktop computer or laptop, a Blu-ray or DVD player, document camera, video conferencing system, media player or set-top box, or even a game console. How video is distributed and displayed is 
equally as diverse.

**Distribution** systems, such as AV extenders, distribution amplifiers, switchers, or matrix switchers, organize AV source inputs and manage the dispersal of these signals to a destination. Streaming encoders and decoders make it possible to deliver media over IP networks as well.

**Destination** devices accept the content from the source or distribution system for display, recording, processing, etc. Examples of destination devices include projectors, flat panel displays, smart phones, and tablets.
## Color Bars

***Color Bars*** are a test pattern generated by a camera, source device, or test generator to provide a reference for examining video signals or validating signal path. If the signal has been altered by recording, transmission, signal extension, or technical issue, adjustments can be made at the display or corrections made to the system to bring it back within specification.
## Display Technology

A display device provides the means to receive video signals and produce the images as visual content. As video signals have transitioned from analog to digital formats, the technology used to display those signals has also evolved. 

Analog displays have given way to a multitude of digital options for presenting visual information in HD, 4K and beyond.
### Cathode Ray Tube - CRT

The earliest form of display technology is the Cathode Ray Tube that fired a stream of electrons onto a phosphor-coated screen, causing it to glow as the image was "drawn" onto the screen. In 2007, demand for CRTSs dropped dramatically and by 2010 production of these displays had ended.
### Plasma Display Panels

***Plasma Display Panels*** - PDP or "Plasmas" illuminate tiny cells with electrically charge ionized gases to create an image. From the mid-1990's to early 2000's, plasmas were the primary high-definition display for resolutions from 1024-1024 up to 1920x1080. 

The last large shipments of plasma televisions occurred in 2010, and declined rapidly, until 2014, when all manufacturers discontinued production of plasma televisions. 
## Projector Technology

When an application requires image larger than can be provided with a direct-view display, the projector is a common AV solution. 

Projectors *project a ray of light through a lens to display an image onto a surface.*

They are compatible with a variety of video sources and vary in resolution much like a television. It is the technology within a projector, the light source, along with distance that will 
determine the quality and dimension of a projected image.

There are several types of projectors to project or reflect their light source. 

- **LCD Projectors**: pass a beam of light to three mirrors that reflect blue, red and green signals to an LCD panel. The pixels are arranged and combined in a prism to create a single image.
- **Digital Light Processing**: DLP technology uses tiny mirrors to represent a single pixel, and a spinning color wheel to create an image.
- **Laser Projectors**: emit laser light that retains a single tight beam over distance, consumes less power, increases brightness, and delivers wider color gamut, more precise color saturation, immediate on/off time, and a longer source life. 
# Elements of Video

## Color Components

There are several factors that determine video quality to give the best viewing experience possible. For example, going from standard def to high def, or Ultra-High Definition - UHD (4K or 8K) will improve image quality, especially on a large display. 

Color space and chroma sampling can also have an effect on the quality of the image. 

Let's take a look at the color elements of a video signal, and how they can have an impact on the image.
### RGB

The smallest component of a digital image is a pixel. These components are composed of individual **Red, Green and Blue - RGB** additive color elements, which are the primary colors. A variety of other color possibilities can be derived from the combination of RGB. Mixing two primary colors creates the secondary colors of Cyan, Magenta and Yellow - CMY. The CMYK color model used in printing consists of the K color which is black.
### YUV

For human vision, the physical pathways of the cortex of the brain treat brightness and color separately. Perception of shape and form are primarily based on brightness, not color. This became a central motivation behind early forms of video compression. 

Video signals would be separated into a brightness channel (luma) and two-color channels (chroma).

With this understanding, the advantage of YUV is that some of the chrominance channel information can be discarded in order to reduce bandwidth and provide savings in storage resources. 
### Chroma Subsampling

***Chroma Subsampling*** is a form of compression that reduces the resolution of the color (chrominance) channels with respect to the resolution of brightness (luminance). This type of compression allows for large savings in the amount of data that is transmitted.

However, chroma subsampling effectively decreases color resolution and saturation, sometimes causing visible signal degradation near the edges of sharp color transitions, and fine detailed patterns.
## Drawing an Image

Standard analog video signals were initially displayed using CRT technology. To accomplish this, horizontal and vertical sync signals controlled the way the image was drawn on the screen. Starting at the upper left of the screen, a left-to-right and top-to-bottom scheme is used to draw images on the screen. 

***Horizontal Sync Signal*** indicates the start of every line and notifies to refresh it. After the first row is drawn, it jumps to the next line each time a horizontal sync pulse is encountered and moves sequentially to the last line. 

- After all lines are drawn, the process of refreshing the screen begins.

***Vertical Sync Signal*** notifies that a new image is to be displayed starting at the upper left corner of the screen.

The video signals are drawn continuously at a fixed number of times every second, known as refresh rate.
## Interlaced Scanning

In traditional TV systems, **Interlaced Scanning** was used to minimize the perception of flicker as the screen updated. Images were drawn in two alternate fields as odd and even numbered lines. 

The first pass displays the first and all subsequent odd numbered lines.

The second pass displays the second line followed by all even numbered lines.

The two fields comprised an entire image. Various interlaced modes included 480i, 576i and 1080i.
## Progressive Scanning

Most modern video signals incorporate progressive scanning as opposed to interlaced scanning. ***Progressive Scanning*** presents moving images by drawing each line sequentially from left to right. A full image frame is created in one pass from top to bottom.

All digital displays use progressive scan modes that display twice as many frames as interlaced in the same amount of time. SO if the signal being sent is interlaced, the display will covert it to progressive scan. This reduces noticeable flickering, visual artifacts and interline twitter, making the motion appear smoother and more realistic.

Common progressive modes include 480p, 720p, and 1080p. The resolution from 720p is 1280x720 progressive, and the resolution for 1080p is 1920x1080 progressive. 
## TVs and Computers

For television displays and computer monitors, there are two items to note. The first being the actual dimension of the screen, which is the corner-to-corner measurement, such as a 65" display. The second is resolution, and the basis for video quality standards.

**Standard Definition Television** - SDTV
- 480i based on NTSC
- 576i based on PAL & SECAM

**High-Definition Television - HDTV**
- 1280x720
- 1920x1080i and 1920x1080p

**Ultra High-Definition Television - UHDTV**
- 4K - 3840x2160p (4096x2160)
- 8K - 7680x4320p
## Frame and Data Rate

### Frame Rate

A frame is a *single image*, and when combined with other still images, achieves the illusion of natural motion to form video. The **Frame Rate** is the number of images displayed in one second, measured in Frames Per Second (fps). The more frames per second results in smoother motion playback.

For example, 4K/60 indicates the content has a 4K resolution output at 60fps, which is 60 complete images displayed per second.
### Refresh Rate

**Refresh Rate** is the frequency at which video frames are refreshed every second, measured in Hertz (Hz). This is how often the display can redraw the screen to show a new image.

For example, a refresh rate of 60 Hz means that a display can redraw the entire screen 60 times in one second. When refresh rates are increased, motion blur, flicker or image tearing issues are reduced.
### Pull Down

**Pull Down** is used in the filmmaking and television post-production process when transferring film to compensate for the difference in frame rates between the film content and the video medium. 

For example, since film is shot at 24 frames per second (1 frame displayed every 24th of a second), in order to display film on a display operating at 60 Hz, the original 24 frames must be converted to 60 fields by a process known as 3:2 pulldown. 

- The 3:2 pulldown process adda a third video field that is added to every second video frame. 

Other pulldown methods, such as 2:2 or 24:1, are used depending on the format of the target medium or TV system.
# Video Signals

## Analog Overview

We once lived in a world where visual media such as television and prerecorded content was created and delivered using analog technology.

Early cameras captured images as electrical signals consisting of constantly fluctuating voltage; the brighter an image was a a particular point in a scene, the higher the voltage produced by the camera. These analog signals were sent to broadcast and recording equipment, and ultimately TV sets, via analog interfaces.

Although analog technology is obsolete in many sectors, you may still encounter analog equipment that requires service and support, or an upgrade. Extron provides products and solutions that support legacy, analog-based systems, as well as products that can integrate analog technology into digital infrastructures.

## Analog Signal Interfaces

A ***Composite*** video interface delivers standard definition video at a resolution of 480i or 576i. The vide information combines the complete video signal and Sync into one channel.

There are many types of `Component` video signals:

- **S-Video** - separates the luminance (Y) and chrominance (C) color signals into two separate channels to carry video at 480i or 576i
- **Component YUV** - A system that defines color via one luminance value and two color-difference chrominance signals. `YPbPr` separates the brightness and color into three signals:
	- Y - Luminance (brightness)
	- Pb - Color difference between blue and luma (color)
	- Pr - Color difference between red and luma (color)
- **RGB**
	- RGBHV - Red, Green, Blue signals carry individual colors for each pixel, with separate horizontal and vertical sync signals. **Video Graphics Array** - VGA are 15-pin connectors that carry RGBHV signals.
	- RGBS - Carries RGB separately, combines horizontal and vertical sync signals into what is called composite sync.
	- RGsB - Carries RGB separately, combines sync to Green signal
## Digital Overview

In the 1980's, standard definition component video was introduced as an uncompressed digital format primarily for large television networks and in video production studios.

This digital information is a discrete binary format (zero or one) where each bit represents two non-continuous amplitudes. Every frame is a bitmap digital image that comprises a raster of pixels with the ability to copy and store content with no degradation in quality. 

This medium is quite the contrast to analog systems where a copy made from a master tape experiences a loss in quality, and a duplicate made from that copy experiences further signal degradation. 
## Analog to Digital Transition

As the requirement for an all-digital infrastructure began to develop, there was a prolonged transition period for displays to improve as well. In the late 1990's, computer graphics cards were created to support digital connectivity using DVI. This signal type was introduced as a direct digital interface for sources and displays, which allowed for digital video to transmit with greater processing power and improved image quality.

However, regardless of this new digital format, VGA connections still remained prevalent on many graphics cards, laptops and displays. As a result, interfacing between computers and displays often required additional components to convert digital signals to analog, or analog signals to digital, which could degrade or alter the siganl.

The HDMI interface was ultimately adopted by the computer industry for digital connections between PC and display, delivering improved image quality and higher resolutions than VGA. 

## Color Bit Depth

As discussed earlier, the smallest element of a digital image is a pixel. ***Color Bit Depth*** indicates the number of bits used to represent the color value of a single pixel. 

For example, a pixel with a color bit depth of 24 bits uses 8-bits per red, green and blue color channel. This allows for 256 different combinations, or *intensity values*, for each primary color. 

The image below shows how three different values of red, green and blue combine to set the color of a pixel. 
## Digital Signal Interfaces

The purpose of a cable and it's associated interface is to physically transport the signal being delivered. There are many types of digital video interfaces. Let's dive into each of the common interface options for digital signals. 
### Serial Digital Interface - SDI

***SD-SDI*** is the serial transmission for component, 480i video over a single standard RG59 or RG6 coaxial cable. SD-SDI was primarily utilized on professional broadcast and video production equipment for bit rates up to 270 Mbps.

***HD-SDI*** is the high definition version of SDI for carrying component, 720p and 1080i video over one coaxial cable. HD-SDI is used for video connections at 1.5 Gbps, supporting HD video, multi-channel audio, closed caption data, test signals, metadata, and error handling. 

- 3G-SDI - 3Gbps, 1080p/60
- 6G-SDI - 6 Gbps, 4k/30
- 12G-SDI - 12 Gbps, 4k/60
- 24G - 24 Gbps, 8K/120
### Digital Video Interface - DVI

***Digital Video Interface*** is a digital connector used for video sources and displays, primarily for computers and graphics card manufacturers. The various DVI interfaces include:

- DVI-A carries analog signals but does experience some quality loss in the digital to analog conversion. Supports up to 1080p/60 resolution. 
- DVI-D carries digital signals between video sources and displays (DVI computer to DVI monitor)
	- Single-Link 1920x1080 @ 60 Hz and 1080p/60
	- Dual Link - 2560x1600
- DVI-I is an integrated cable that carries both digital and analog signals simultaneously. 
	- The maximum analog resolution is 1080p/60
	- The digital signal is single link - 1920x1200 @ 60 Hz and 1080p/60
### High Definition Multimedia Interface - HDMI

***High Definition Multimedia Interface*** is a digital transmission interface for video, multi-channel audio, and control data to HDMI-compatible televisions, monitors, projectors and other displays. 

- Standard - 1080p/60 75'
- High Speed - 4K/30 (4096x2160), 35' to 50'
- Premium - 4K/60 (4096x2160), 25'
- Specifications
	- 1.4 1080/120, 4K at 24, 25, 30 Hz, 10.2 Gbps
	- 2.0 4K/60, 18 Gbps
	- 2.1 4k/120, 8K/120, 10K/120, 48 Gbps

HDMI Supports High-Bandwidth Digital Content Protection, to prevent high-value content from being copied or displayed on devices not HDCP compliant. 
### DisplayPort - DP

***DisplayPort*** is a digital-only video transmission interface deigned for the computer hardware industry. It renders as micro packets similar to network data. The four serial data lanes carry signals with a pixel clock as the timing reference.

DisplayPort also supports the following elements:
- 32 digital audio channels
- HDCP and DisplayPort Content Protection - DPCP
- Differential AUX channel proivides EDID communication
- Specifications
	- 1.2 - 4K/60, 17.28 Gbps
	- 1.3 - 4K/120, 8K/30, 32.4 Gbps
	- 1.4 - 8K/60, 4K/120, 32.4 Gbps
### Universal Serial Bus - USB

***Universal Serial Bus*** multipurpose ports and cables are used for connecting various peripheral devices such as a mouse, keyboard, camera, printers, flash drives, disk drives and more to computers for communication. It has largely replaced a variety of interfaces, such as serial and parallel ports, as well as for power to charge portable devices.

There are a variety of USB connectors, including Type A, Type B, Type C, Mini, and Micro. USB Type-C is a slim, reversible connector that transfers high-bandwidth audio, video, data and power at speeds of up t 10 Gbps between laptops, smartphones, and tablets. To power these devices, USB-C also supports fast charging that delivers up to 100 watts.

With the combination of data transfer and communications capability, along with power delivery and AV, the USB-C connector is becoming the most essential connector on many computers, mobile devices and cameras.
## Additional Video Interfaces

***Thunderbolt*** interfaces connect computers to external devices, along with providing DC power. TB1 and TB2 use the same connector as Mini DisplayPort - MDP, while TB3 uses the USB Type-C connector.

The ***Lightning*** connector is used to connect Apple devices such as iPhones, iPads, and iPods to host computers, external monitors, cameras, USB battery chargers, and other peripherals.
# Transporting Video

## Communication Protocols

> To ensure optimum presentation of content, AV devices communicate with one another to establish compatible signal resolution, frame rate, audio format, and other parameters. 

Let's take a look at some standard communication signals that are used between devices.

### Transition Minimized Differential Signaling - TMDS

The all-digital video transmission standard used as the core technology in DVI and HDMI interfaces is `Transition Minimized Differential Signaling - TMDS`. There are four channels for Red, Green, Blue and Clock. TMDS is an algorithm that reduces electromagnetic interference (EMI).

The HDMI interface incorporates additional connections to allow the source and display to exchange information. These include DDC, HPD, and CEC communication lines.
### Display Data Channel - DDC

The communication protocol channel used for transmitting EDID information between the display and source to negotiate format and resolution.
### Extended Display Identification Data - EDID

Information about a display's supported features and capabilities is defined as its ***Extended Display Identification Data - EDID***. It is what *enables a device to know what kind of monitor is connected, by providing the name, ID number, model number, serial number, display size, and aspect ratio.*

EDID is defined by standards that are published from the VESA organization.
### Hot Plug Detect - HPD

The ***Hot Plug Detect*** is a communication mechanism that makes the source device aware that it has been connected to or disconnected from the sink device. For example, it detects the presence of a display.
### Consumer Electronics Control - CEC

***Consumer Electronics Control*** is a serial protocol embedded into HDMI signals that allows devices to communicate. CEC can modify device settings when the status of another device changes.
## Compression and Latency

As with any transmission, bandwidth can be a limiting factor in the delivery of video signals. One way to alleviate bandwidth requirements is to reduce the amount of video signal data. This is known as **Compression**. 

- Compression enables higher resolution video signals to be transmitted over existing lower bandwidth infrastructures.

	- ***Mathematically Lossless Compression*** allows the original data to be perfectly reconstructed from its compressed state.
	- ***Visually Lossless Compression*** is where the decoded image is not reconstructed mathematically and appears the same as the original. 
	- ***Lossy Compression*** permits reconstruction only at an approximation of the original data, though compression rate and file sizes are improved.

The amount of time it takes to deliver a video from source to display is called ***Latency***. The impact latency has on a system will vary from application to application. A few hundred milliseconds of latency may be a detriment for real-time, direct delivery needs, while several seconds of latency may be irrelevant in an on-demand streaming application.

Some of the common causes for latency are signal processing such as scaling, encoder processing, decoder processing, network latency, and processing within the display itself.
## Video Standards Overview

> In the video world, a variety of standards exist to address the needs for interconnected and interoperability of equipment and applications. The ability to comply with these standards ensures devices can be properly set up to allow for successful encoding, calibration, formatting, transmission, reception, and display of video signals.
### Motion Picture Standards

Two organizations, SMPTE and DCI, define the majority of the standards used in professional media, entertainment technology and motion imaging.

#### SMPTE - Society of Motion Pictures & Television Engineers

An international professional organization responsible for over 600 standards, including SDI, along with recommended practices, and engineering guidelines. The SMPTE standards include TV production and transmission, motion picture filmmaking, digital cinema, audio recording and broadcast formats.

SMPTE is responsible for developing the color bar test pattern, which is used to examine video signals. If the signal has been altered by recording or transmission, adjustments can be made to bring it back to specification.
#### Digital Cinema Initiatives - DCI

***Digital Cinema Initiatives - DCI*** is a joint project comprised of major motion picture studios to establish specifications for digital cinema systems. Their documented specifications ensure a high level of technical performance, reliability and quality. 
### Analog Television Broadcast Standards

> The analog television video formats used around the world adhere to NTSC, PAL and SECAM standards.

#### National Television Systems Committee - NTSC

***National Television Systems Committee*** is the analog color video standard for North America, parts of Central America, Japan and South Korea. NTSC parameters deliver 525 lines at 60 Hz (60 interlaced fields, or 30 frames per second).

- 480 lines of active/visual video
- 45 lines of blanking/sync intervals

In 2010, the United States terminated NTSC analog broadcasts except for lower power television stations and closed circuit applications.
#### Phase Altering Line - PAL

***Phase Altering Line - PAL*** is the analog television standard used in Western Europe, Australia, Africa, the Middle East, and parts of South America. 

"Phase Altering Line" describes the color encoding system used by the standard. PAL delivers 625 lines at 50 Hz (50 interlaced fields, 25 fps)

- 576 lines of active/visual video
- 49 lines of blanking/sync visuals
#### Sequential Couleur avec Memoire - SECAM

Analog and television standard is used in France, North Africa, Russia, Saudi Arabia, and Eastern Europe. SECAM is similar to PAL at 625/50 but color transmission is performed differently. 
### Digital Television Broadcast Standards

> Standards around the world for digital broadcast systems are ATSC, DVB, ISDB and DTMB.

#### Advanced Television Systems Committee

Standards for digital television transmission replaced the analog NTSC standards used in North America, Mexico and Canada. ATSC established technical standards for HDTV that supports video resolutions of 720p and 1080i.
#### Digital Video Broadcasting - DVB

Standards adopted in Europe, Asia and Africa, are open technical standards for the delivery of digital TV content by terrestrial, cable, satellite and mobile communication systems.
#### Integrated Services Digital Broadcasting - ISDB

Japanese standard for the transport stream of digital television and audio coding (MPEG-2 or H.264).
#### Digital Terrestrial Multimedia Broadcast - DTMB

Digital standard for mobile and fixed reception used in China, Cuba and Hong Kong.
## The Need for Digital Content Protection

Copyright protection is a challenge for all media, from print to video. With the development of digital video came the ability to create perfect duplicates with no generational loss, regardless of how many times the content was copied. 

This ease, affordability, and speed of duplication and distribution heightened the demand for piracy protection.

A system of copyrights and licensing controls how each business in the value chain is compensated, and the proliferation of digital technologies threatens this system by making copyrights difficult to enforce. Over the years, rights holders have devised many methods to prevent and deter unauthorized copying and distribution of content.

- For example, disrupting the recording or encrypting DVDs to prevent digital copies. 

The increased popularity of high definition video makes content protection more important than ever. If high quality HD video can be perfectly copied by unauthorized means, it presents a greater loss to copyright holders than if material in lower resolution formats such as VHS or DVD were compromised. 
### HDCP Protection

***High Bandwidth Digital Content Protection*** was implemented by Digital Rights Management, DRM to prevent unauthorized copying of protected content. This encryption protocol is applied between video sources and displays, with the following versions having been implemented:

- Version 1.0 applied DVI
- Version 1.1 applied to HDM
- Version 1.3 added support for DisplayPort
- Version 2.0 HDCP became interface-dependent
- Version 2.2 encryption of 4K sources.

HDCP restricts what the end user can do with protected content by limiting the number of simultaneous displays for content protected video playback, disallowing copying or recording, and disabling analog outputs. HDCP-compliant devices use an authentication and key exchange procedure before video and audio is presented.




